import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


def chat(model_dir="outputs/checkpoints", max_new_tokens=100):
    """
    ä¸­æ–‡å¯¹è¯æ¨¡å‹æ¨ç†è„šæœ¬
    """
    print("ğŸ”§ æ­£åœ¨åŠ è½½æ¨¡å‹...")

    # åŠ è½½tokenizerå’Œæ¨¡å‹
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None
        )
        model.eval()

        # ç¡®ä¿pad_tokenè®¾ç½®æ­£ç¡®
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print(f"ğŸ“± è®¾å¤‡: {'CUDA' if torch.cuda.is_available() and model.device.type == 'cuda' else 'CPU'}")
        print(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return

    print("\nğŸ¤– ä¸­æ–‡å¯¹è¯æ¨¡å‹å·²åŠ è½½ï¼Œè¾“å…¥ 'exit' é€€å‡º")
    print("ğŸ’¡ æç¤ºï¼šè¾“å…¥ 'clear' æ¸…é™¤å¯¹è¯å†å²\n")

    # ç»´æŠ¤å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
    conversation_history = []

    while True:
        try:
            user_input = input("\nä½ ï¼š").strip()

            if user_input.lower() in ["exit", "quit", "q", "é€€å‡º"]:
                print("ğŸ‘‹ å†è§ï¼")
                break

            if user_input.lower() in ["clear", "æ¸…é™¤"]:
                conversation_history = []
                print("ğŸ—‘ï¸ å¯¹è¯å†å²å·²æ¸…é™¤")
                continue

            if not user_input:
                continue

            # æ„å»ºè¾“å…¥prompt - å…³é”®æ”¹è¿›
            # æ–¹å¼1: ç®€å•çš„æŒ‡ä»¤æ ¼å¼
            prompt = f"ç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š"

            # æ–¹å¼2: å¦‚æœéœ€è¦å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
            # if conversation_history:
            #     context = "\n".join([f"ç”¨æˆ·ï¼š{h['user']}\nåŠ©æ‰‹ï¼š{h['assistant']}" for h in conversation_history[-3:]])  # ä¿ç•™æœ€è¿‘3è½®å¯¹è¯
            #     prompt = f"{context}\nç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š"
            # else:
            #     prompt = f"ç”¨æˆ·ï¼š{user_input}\nåŠ©æ‰‹ï¼š"

            # tokenizeè¾“å…¥
            inputs = tokenizer(
                prompt,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=256 // 2  # ä¸ºç”Ÿæˆç•™å‡ºç©ºé—´
            )

            if torch.cuda.is_available():
                inputs = {k: v.to(model.device) for k, v in inputs.items()}

            # ç”Ÿæˆå›å¤
            print("ğŸ¤” AIæ­£åœ¨æ€è€ƒ...")
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.8,  # ç¨å¾®æé«˜åˆ›é€ æ€§
                    top_p=0.85,  # ç¨å¾®é™ä½ï¼Œå‡å°‘éšæœºæ€§
                    top_k=50,  # æ·»åŠ top_ké‡‡æ ·
                    repetition_penalty=1.1,  # å‡å°‘é‡å¤
                    no_repeat_ngram_size=3,  # é¿å…é‡å¤3-gram
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )

            # è§£ç å¹¶å¤„ç†å›å¤
            full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

            # æå–AIå›å¤éƒ¨åˆ†
            if "åŠ©æ‰‹ï¼š" in full_response:
                reply = full_response.split("åŠ©æ‰‹ï¼š")[-1].strip()
            else:
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ†éš”ç¬¦ï¼Œç§»é™¤è¾“å…¥éƒ¨åˆ†
                reply = full_response[len(prompt):].strip()

            # æ¸…ç†å›å¤
            reply = clean_response(reply)

            if reply:
                print(f"AIï¼š{reply}")

                # ä¿å­˜å¯¹è¯å†å²ï¼ˆå¯é€‰ï¼‰
                # conversation_history.append({
                #     "user": user_input,
                #     "assistant": reply
                # })
            else:
                print("AIï¼šæŠ±æ­‰ï¼Œæˆ‘æ²¡èƒ½ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·æ¢ä¸ªæ–¹å¼æé—®ã€‚")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç”¨æˆ·ä¸­æ–­ï¼Œå†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ ç”Ÿæˆè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            continue


def clean_response(response):
    """
    æ¸…ç†AIå“åº”
    """
    if not response:
        return ""

    # ç§»é™¤å¯èƒ½çš„æç¤ºè¯æ®‹ç•™
    unwanted_prefixes = ["ç”¨æˆ·ï¼š", "åŠ©æ‰‹ï¼š", "AIï¼š", "ä½ ï¼š"]
    for prefix in unwanted_prefixes:
        if response.startswith(prefix):
            response = response[len(prefix):].strip()

    # åœ¨å¥å·ã€é—®å·ã€æ„Ÿå¹å·å¤„æˆªæ–­ï¼ˆé¿å…ä¸å®Œæ•´å¥å­ï¼‰
    for punct in ["ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?"]:
        if punct in response:
            # ä¿ç•™åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¥å­
            sentences = response.split(punct)
            if len(sentences) > 1 and sentences[-1].strip() == "":
                response = punct.join(sentences[:-1]) + punct
                break

    # ç§»é™¤å¤šä½™çš„æ¢è¡Œå’Œç©ºæ ¼
    response = response.replace("\n", " ").strip()

    # æˆªæ–­è¿‡é•¿å›å¤
    if len(response) > 200:
        response = response[:200] + "..."

    return response


def test_specific_prompts():
    """
    æµ‹è¯•ç‰¹å®šçš„æç¤ºè¯æ ¼å¼
    """
    model_dir = "outputs/checkpoints"

    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir)
        model = AutoModelForCausalLM.from_pretrained(model_dir)
        model.eval()

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        test_cases = [
            "ä½ å¥½",
            "ä½ æ˜¯è°ï¼Ÿ",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹Pythonç¼–ç¨‹è¯­è¨€"
        ]

        for test in test_cases:
            prompt = f"<|user|>{test}<|assistant|>"  # å°è¯•ä¸åŒçš„æ ¼å¼
            inputs = tokenizer(prompt, return_tensors="pt")

            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id
            )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print(f"æµ‹è¯•: {test}")
            print(f"ç»“æœ: {response}\n")

    except Exception as e:
        print(f"æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    # è¿è¡Œä¸»å¯¹è¯ç¨‹åº
    chat()

    # å¦‚æœæƒ³è¦æµ‹è¯•ä¸åŒæ ¼å¼ï¼Œå–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
    # print("\n" + "="*50)
    # print("æµ‹è¯•ä¸åŒçš„promptæ ¼å¼:")
    # test_specific_prompts()