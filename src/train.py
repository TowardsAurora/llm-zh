import json
import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments
from dataset import AlpacaDataset


def main():
    with open("configs/train_config.json", "r", encoding="utf-8") as f:
        config = json.load(f)

    model_name = config["model_name"]
    train_file = config["train_file"]
    output_dir = config["output_dir"]
    max_seq_length = config["max_seq_length"]

    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ä¸ºä¸­æ–‡ä¼˜åŒ–pad_tokenè®¾ç½®
    if tokenizer.pad_token is None:
        # å¯¹äºä¸­æ–‡ï¼Œå»ºè®®æ·»åŠ ä¸“é—¨çš„pad_tokenè€Œä¸æ˜¯ä½¿ç”¨eos_token
        tokenizer.add_special_tokens({'pad_token': '[PAD]'})

    model = AutoModelForCausalLM.from_pretrained(model_name)

    # å¦‚æœæ·»åŠ äº†æ–°çš„ç‰¹æ®Štokenï¼Œéœ€è¦è°ƒæ•´æ¨¡å‹çš„embeddingå±‚
    if tokenizer.pad_token == '[PAD]':
        model.resize_token_embeddings(len(tokenizer))

    # åŠ è½½æ•°æ®é›†
    train_dataset = AlpacaDataset(train_file, tokenizer, max_length=max_seq_length)

    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_dataset)}")
    print(f"ğŸ”¤ è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}")

    # ä¼˜åŒ–åçš„è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=config["per_device_train_batch_size"],
        per_device_eval_batch_size=config["per_device_eval_batch_size"],
        num_train_epochs=config["num_train_epochs"],
        learning_rate=config["learning_rate"],
        warmup_steps=config["warmup_steps"],
        logging_steps=config["logging_steps"],
        save_steps=config["save_steps"],
        save_total_limit=config["save_total_limit"],
        fp16=torch.cuda.is_available(),
        logging_dir=os.path.join(output_dir, "logs"),
        report_to="none",

        # ä¸ºä¸­æ–‡è®­ç»ƒæ·»åŠ çš„ä¼˜åŒ–å‚æ•°
        gradient_accumulation_steps=4,  # å¢åŠ æœ‰æ•ˆbatch size
        weight_decay=0.01,  # é˜²æ­¢è¿‡æ‹Ÿåˆ
        lr_scheduler_type="cosine",  # ä½™å¼¦å­¦ä¹ ç‡è°ƒåº¦
        remove_unused_columns=False,
        dataloader_pin_memory=True,
        save_safetensors=True,
        load_best_model_at_end=True,
        metric_for_best_model="loss",
        greater_is_better=False,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        # æ·»åŠ æ•°æ®æ•´ç†å™¨æ¥å¤„ç†å˜é•¿åºåˆ—
        data_collator=lambda data: {
            'input_ids': torch.stack([f['input_ids'] for f in data]),
            'attention_mask': torch.stack([f['attention_mask'] for f in data]),
            'labels': torch.stack([f['labels'] for f in data])
        }
    )

    # å¼€å§‹è®­ç»ƒ
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    trainer.train()

    # ä¿å­˜æ¨¡å‹
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {output_dir}")

    # è®­ç»ƒåç«‹å³æµ‹è¯•
    print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•...")
    test_model(model, tokenizer)


def test_model(model, tokenizer):
    """è®­ç»ƒåçš„å¿«é€Ÿæµ‹è¯•"""
    model.eval()
    test_inputs = ["ä½ å¥½", "ä»Šå¤©å¤©æ°”", "è¯·ä»‹ç»ä¸€ä¸‹"]

    for test_input in test_inputs:
        inputs = tokenizer.encode(f"ç”¨æˆ·ï¼š{test_input}\nåŠ©æ‰‹ï¼š", return_tensors="pt")

        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_new_tokens=30,
                do_sample=True,
                temperature=0.8,
                top_p=0.9,
                repetition_penalty=1.2,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"è¾“å…¥: {test_input}")
        print(f"è¾“å‡º: {response}\n")


if __name__ == "__main__":
    main()